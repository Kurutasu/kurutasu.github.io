---
layout: post
title: 作成中）統計的学習の基礎 第2章まとめ
updated: 2017-08-26 20:00 
categories:
 - machine learning
---

[Trevor Hastie，統計的学習の基礎 ―データマイニング・推論・予測―](http://www.kyoritsu-pub.co.jp/bookdetail/9784320123625)の第2章についてメモする．本書のメモ一覧は[こちら](https://haltaro.github.io/2017/08/26/the-elements-of-statistical-learning)．

# 2. 教師あり学習の概要

## 2.1 導入

* 入力は，統計学では**予測変数**（predictor）や**独立変数**（independent variable）と，パターン認識では**特徴**（feature）と呼ばれる．
* 出力は，**応答変数**（response variable）や**従属変数**（dependent variable）と呼ばれる．

## 2.2 変数の種類と用語

* **量的変数**（quantitative variable）と**質的変数**（qualitative variable）がある．質的変数は，**カテゴリ型変数**（categorical variable），**離散変数**（discrete variable），あるいは**因子**（factor）と呼ばれることがある．
* ３つめの変数として，**順序付きカテゴリ型変数**（ordered categorical variable）がある．これは，値の順序にのみ意味があり，計量としての意味はない．
* 本書では，入力変数を$$X$$，量的な出力変数を$$Y$$，質的な出力変数を$$G$$と表す．$$X$$の$$i$$番目の観測値は，ベクトルでもスカラーでも，イタリック体で$$x_i$$と表す．一方で，全てのベクトル$$x_i$$の$$j$$番目の要素をまとめたベクトルは，立体で$$\mathrm{x}_j$$と表す．
* 学習とは，入力ベクトル$$X$$が与えられたとき，出力$$Y$$の良い予測値$$\hat{Y}$$を求めること．

## 2.3 予測のための二つの簡単なアプローチ：最小二乗法と最近傍法

* **線形モデル**は，入力ベクトル$$X^{T}=\left(X_1, X_2, ..., X_p\right)$$に対し，出力$$Y$$を$$\hat{Y}=X^{T}\hat{\beta}$$で予測するモデルである．
* 線形モデルにおける訓練データの当てはめには，**最小二乗法**が用いられる．最小二乗法では，残差二乗和$$\mathrm{RSS}(\beta)=\left(\mathbf{y} - \mathbf{X} \beta \right)^{T}\left(\mathbf{y} - \mathbf{X} \beta \right)$$を最小化する$$\beta$$を求める．ここで，$$\mathbf{X}$$は各業が入力ベクトル$$X$$である$$N \times p$$行列，$$\mathbf{y}$$は各行が出力$$y$$である$$N$$次元ベクトルである．
* 上式を$$\beta$$について偏微分して導出される$$\mathbf{X}^{T}\left(\mathbf{y}-\mathbf{X}\beta\right)=0$$を**正規方程式**と呼ぶ．これを解くと，$$\hat{\beta} = \left( \mathbf{X}^{T} \mathbf{X} \right)^{-1} \mathbf{X}^{T} \mathbf{y}$$を得る．
* **最近傍法**は，入力$$x$$に対し，出力$$Y$$を$$\hat{Y}=\frac{1}{k}\sum_{x_{i} \in N_{k}(x)}y_{i}$$で予測するモデルである．ここで，$$N_{k}(x)$$は訓練データのうち$$x$$に近い$$k$$個の点$$x_{i}$$によって定義される近傍集合．分類問題に$$k=1$$の最近傍法を用いると，訓練データの[**ボロノイ分割**](https://ja.wikipedia.org/wiki/%E3%83%9C%E3%83%AD%E3%83%8E%E3%82%A4%E5%9B%B3)が得られる．
* 最近傍法のパラメータ$$k$$を最小二乗法で求めることはできない．
* 線形モデルは分散が小さく，バイアスが大きい．一方で最近傍法は，分散が大きく，バイアスが小さい．
* 線形モデルおよび最近傍法の拡張：
  * **カーネル平滑化**：最近傍法では重みが0か1かの離散値だったが，カーネル平滑化では連続値に拡張する．
  * **局所線形モデル**：最近傍法では局所領域$$N$$が定数モデルだったが，局所線形モデルでは線形モデルに拡張する．
  * **基底展開**：入力モデルを基底展開することで，線形モデルを拡張する．
  * **射影追跡法**や**ニューラルネットワーク**は，非線形返還と線形モデルを結合したものと考えられる．

## 2.4 統計的決定理論

## 2.5 高次元での局所的手法

## 2.6 統計モデル，教師あり学習，関数近似

## 2.7 構造化回帰モデル

## 2.8 制限付き推定法

## 2.9 モデルの選択と，バイアスと分散のトレードオフ
