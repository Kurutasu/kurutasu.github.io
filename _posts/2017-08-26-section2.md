---
layout: post
title: 作成中）統計的学習の基礎 第2章まとめ
updated: 2017-08-26 20:00 
categories:
 - machine learning
---

[Trevor Hastie，統計的学習の基礎 ―データマイニング・推論・予測―](http://www.kyoritsu-pub.co.jp/bookdetail/9784320123625)の第2章についてメモする．本書のメモ一覧は[こちら](https://haltaro.github.io/2017/08/26/the-elements-of-statistical-learning)．

# 2. 教師あり学習の概要

## 2.1 導入

* 入力は，統計学では**予測変数**（predictor）や**独立変数**（independent variable）と，パターン認識では**特徴**（feature）と呼ばれる．
* 出力は，**応答変数**（response variable）や**従属変数**（dependent variable）と呼ばれる．

## 2.2 変数の種類と用語

* **量的変数**（quantitative variable）と**質的変数**（qualitative variable）がある．質的変数は，**カテゴリ型変数**（categorical variable），**離散変数**（discrete variable），あるいは**因子**（factor）と呼ばれることがある．
* ３つめの変数として，**順序付きカテゴリ型変数**（ordered categorical variable）がある．これは，値の順序にのみ意味があり，計量としての意味はない．
* 本書では，入力変数を$$X$$，量的な出力変数を$$Y$$，質的な出力変数を$$G$$と表す．$$X$$の$$i$$番目の観測値は，ベクトルでもスカラーでも，イタリック体で$$x_i$$と表す．一方で，全てのベクトル$$x_i$$の$$j$$番目の要素をまとめたベクトルは，立体で$$\mathrm{x}_j$$と表す．
* 学習とは，入力ベクトル$$X$$が与えられたとき，出力$$Y$$の良い予測値$$\hat{Y}$$を求めること．

## 2.3 予測のための二つの簡単なアプローチ：最小二乗法と最近傍法

* **線形モデル**は，入力ベクトル$$X^{T}=\left(X_1, X_2, ..., X_p\right)$$に対し，出力$$Y$$を$$\hat{Y}=X^{T}\hat{\beta}$$で予測するモデルである．
* 線形モデルにおける訓練データの当てはめには，**最小二乗法**が用いられる．最小二乗法では，残差二乗和$$\mathrm{RSS}(\beta)=\left(\mathbf{y} - \mathbf{X} \beta \right)^{T}\left(\mathbf{y} - \mathbf{X} \beta \right)$$を最小化する$$\beta$$を求める．ここで，$$\mathbf{X}$$は各行が入力ベクトル$$X$$である$$N \times p$$行列，$$\mathbf{y}$$は各行が出力$$y$$である$$N$$次元ベクトルである．
* 上式を$$\beta$$について偏微分して導出される$$\mathbf{X}^{T}\left(\mathbf{y}-\mathbf{X}\beta\right)=0$$を**正規方程式**と呼ぶ．これを解くと，$$\hat{\beta} = \left( \mathbf{X}^{T} \mathbf{X} \right)^{-1} \mathbf{X}^{T} \mathbf{y}$$を得る．
* **最近傍法**は，入力$$x$$に対し，出力$$Y$$を$$\hat{Y}=\frac{1}{k}\sum_{x_{i} \in N_{k}(x)}y_{i}$$で予測するモデルである．ここで，$$N_{k}(x)$$は訓練データのうち$$x$$に近い$$k$$個の点$$x_{i}$$によって定義される近傍集合．分類問題に$$k=1$$の最近傍法を用いると，訓練データの[**ボロノイ分割**](https://ja.wikipedia.org/wiki/%E3%83%9C%E3%83%AD%E3%83%8E%E3%82%A4%E5%9B%B3)が得られる．
* 最近傍法のパラメータ$$k$$を最小二乗法で求めることはできない．
* 線形モデルは分散が小さく，バイアスが大きい．一方で最近傍法は，分散が大きく，バイアスが小さい．
* 線形モデルおよび最近傍法の拡張：
  * **カーネル平滑化**：最近傍法では重みが0か1かの離散値だったが，カーネル平滑化では連続値に拡張する．
  * **局所線形モデル**：最近傍法では局所領域$$N$$が定数モデルだったが，局所線形モデルでは線形モデルに拡張する．
  * **基底展開**：入力モデルを基底展開することで，線形モデルを拡張する．
  * **射影追跡法**や**ニューラルネットワーク**は，非線形返還と線形モデルを結合したものと考えられる．

## 2.4 統計的決定理論

* 確率入力ベクトル$$X \in \mathbb{R}^{p}$$と確率出力変数$$Y \in \mathbb{R}$$が同時確率分布$$\mathrm{Pr}(X,Y)$$に従うと仮定する．目的は，入力$$X$$に対して出力$$Y$$を予測する関数$$f(X)$$を見つけること．
* $$L\left(Y,f(X)\right)$$を**損失関数**（Loss function）とする．最も頻繁に利用されるのは，**二乗誤差損失**（Least square loss）$$L\left(Y,f(X)\right)=\left(Y-f(X)\right)^2$$．
* 期待（二乗）予測誤差（expected prediction error）$$\mathrm{EPE}(f)=\mathrm{E}\left(Y-f(X)\right)$$を最小化する$$f$$は条件付き期待値$$\mathrm{E}(Y\mid X=x)$$であり，これは**回帰関数**とも呼ばれる．
* 最近傍法は，訓練データを用いて条件付き期待値を直接推定する方法と解釈できる．具体的には，$$\hat{f}(x)=\mathrm{Ave}\left(y_i \mid x_i \in N_k(x) \right)$$．ここでは，以下の近似が行われていることに注意すべき：
  * 期待値を標本データの平均で近似している．
  * 点$$x$$に対する条件付けではなく，それを緩和した近傍$$N_k(x)$$で条件付けしている．
  
* 訓練データ数$$N$$および近傍数$$k$$を十分大きくすれば，$$\hat{f}(x)$$は条件付き期待値に収束する．しかし，次元数$$p$$が大きいとき，十分な数のデータを得ることが困難になるし，収束も遅くなる．
* 一方，線形モデル$$f(x)=x^{T}\beta$$のとき，解$$\beta$$は解析的に求められる：$$\beta=[\mathrm{E}(XX^{T})]^{-1}\mathrm{E}(XY)$$．
* 最近傍法も最小二乗法も，条件付き期待値の近似である．最近合法では$$f(x)$$を局所的な定数関数で近似し，最小二乗法では$$f(x)$$を大域的な線形関数で近似している．
* ちなみに，$$L_2$$損失関数を$$L_1$$損失関数に変更すると，解は条件付き中央値$$\hat{f}(x)=\mathrm{median}(Y \mid X=x)$$となる．
* 出力がカテゴリ型変数$$G$$の場合，損失関数は$$K \times K$$行列$$\mathbf{L}$$となる．損失行列$$\mathbf{L}$$の要素$$(k,l)$$は，クラス$$\mathcal{G}_k$$を$$\mathcal{G}_l$$に分類した際のペナルティを表す．損失行列$$\mathbf{L}$$が$$\{0, 1\}$$のとき，$$0/1$$損失関数と呼ばれる．

## 2.5 高次元での局所的手法

## 2.6 統計モデル，教師あり学習，関数近似

## 2.7 構造化回帰モデル

## 2.8 制限付き推定法

## 2.9 モデルの選択と，バイアスと分散のトレードオフ

# 演習問題

合っているかわからないが，一通りトライしてみる．

## 2.1

## 2.2

## 2.3

## 2.4

## 2.5

## 2.6

## 2.7

## 2.8

## 2.9
