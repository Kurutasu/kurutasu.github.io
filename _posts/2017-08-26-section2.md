---
layout: post
title: 作成中）統計的学習の基礎 第2章まとめ
updated: 2017-08-26 20:00 
categories:
 - machine learning
---

[Trevor Hastie，統計的学習の基礎 ―データマイニング・推論・予測―](http://www.kyoritsu-pub.co.jp/bookdetail/9784320123625)の第2章についてメモする．本書のメモ一覧は[こちら](https://haltaro.github.io/2017/08/26/the-elements-of-statistical-learning)．

# 2. 教師あり学習の概要

## 2.1 導入

* 入力は，統計学では**予測変数**（predictor）や**独立変数**（independent variable）と，パターン認識では**特徴**（feature）と呼ばれる．
* 出力は，**応答変数**（response variable）や**従属変数**（dependent variable）と呼ばれる．

## 2.2 変数の種類と用語

* **量的変数**（quantitative variable）と**質的変数**（qualitative variable）がある．質的変数は，**カテゴリ型変数**（categorical variable），**離散変数**（discrete variable），あるいは**因子**（factor）と呼ばれることがある．
* ３つめの変数として，**順序付きカテゴリ型変数**（ordered categorical variable）がある．これは，値の順序にのみ意味があり，計量としての意味はない．
* 本書では，入力変数を$$X$$，量的な出力変数を$$Y$$，質的な出力変数を$$G$$と表す．$$X$$の$$i$$番目の観測値は，ベクトルでもスカラーでも，イタリック体で$$x_i$$と表す．一方で，全てのベクトル$$x_i$$の$$j$$番目の要素をまとめたベクトルは，立体で$$\mathrm{x}_j$$と表す．
* 学習とは，入力ベクトル$$X$$が与えられたとき，出力$$Y$$の良い予測値$$\hat{Y}$$を求めること．

## 2.3 予測のための二つの簡単なアプローチ：最小二乗法と最近傍法

* **線形モデル**は，入力ベクトル$$X^{T}=\left(X_1, X_2, ..., X_p\right)$$に対し，出力$$Y$$を$$\hat{Y}=X^{T}\hat{\beta}$$で予測するモデルである．
* 線形モデルにおける訓練データの当てはめには，**最小二乗法**が用いられる．最小二乗法では，残差二乗和$$\mathrm{RSS}(\beta)=\left(\mathbf{y} - \mathbf{X} \beta \right)^{T}\left(\mathbf{y} - \mathbf{X} \beta \right)$$を最小化する$$\beta$$を求める．ここで，$$\mathbf{X}$$は各業が入力ベクトル$$X$$である$$N \times p$$行列，$$\mathbf{y}$$は各行が出力$$y$$である$$N$$次元ベクトルである．
* 上式を$$\beta$$について偏微分して導出される$$\mathbf{X}^{T}\left(\mathbf{y}-\mathbf{X}\beta\right)=0$$を**正規方程式**と呼ぶ．これを解くと，$$\hat{\beta}=\left( \mathbf(X}^{T} \mathbf{X} \right)^{-1} \mathbf{X}^{T} \mathbf{y}$$を得る．

## 2.4 統計的決定理論

## 2.5 高次元での局所的手法

## 2.6 統計モデル，教師あり学習，関数近似

## 2.7 構造化回帰モデル

## 2.8 制限付き推定法

## 2.9 モデルの選択と，バイアスと分散のトレードオフ
