---
layout: post
title:  "【G検定】チートシート"
updated: 2021-02-18
cover:  "/assets/cover_image.jpg"
subheadline:  "G検定まとめ"
categories: 
- AI
---

# はじめに

* [一般社団法人 日本ディープラーニング協会　G検定](https://www.jdla.org/certificate/general/)

* ディープラーニングG検定に向けた情報整理を行う。

* 構成は[シラバス](https://www.jdla.org/certificate/general/#general_No03)に従い、該当項目には「📘」を付す。

* 参考図書

    [ディープラーニング G検定(ジェネラリスト) 公式テキスト](https://www.amazon.co.jp/dp/4798157554)

* 模擬テスト

    [Study-AI G検定 模擬テスト](http://study-ai.com/generalist/)

# １．📘人工知能（AI）とは（人工知能の定義）

## AIの定義

* 専門家の間で共有されている定義はない。

* 人工知能であるかどうかは「人によって違う」。

* 定義の例

    * 「推論、認識、判断など、人間と同じ知的な処理能力を持つ機械（情報処理システム）」
    
    * 「周囲の状況（入力）によって行動（出力）を変えるエージェント（プログラム）」

    * 🎩松尾豊
    
        「人工的につくられた人間のような知能、ないしはそれをつくる技術」

    * 🎩アーサー・サミュエル

        「明示的にプログラムしなくても学習する能力をコンピュータに与える研究分野」

## 人工知能レベル

* レベル1

    シンプルな制御プログラム。**ルールベース**。
 
* レベル2

    古典的な人工知能。**探索・推論**を行う。知識データを利用する。
 
* レベル3

    [**機械学習**](https://d.hatena.ne.jp/keyword/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92)を取り入れた人工知能。多くのデータから入力・出力関係を学習する。
 
* レベル4   

    [**ディープラーニング**](https://d.hatena.ne.jp/keyword/%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0)を取り入れた人工知能。特徴量による学習を行う。

## AI効果

* [人工知能](https://d.hatena.ne.jp/keyword/%E4%BA%BA%E5%B7%A5%E7%9F%A5%E8%83%BD)の原理がわかると「単純な自動化である」とみなしてしまう人間の心理のこと。

## ロボットとの違い

* [人工知能](https://d.hatena.ne.jp/keyword/%E4%BA%BA%E5%B7%A5%E7%9F%A5%E8%83%BD)では「考える」という、目に見えないものを中心に扱っている。

* [人工知能](https://d.hatena.ne.jp/keyword/%E4%BA%BA%E5%B7%A5%E7%9F%A5%E8%83%BD)ではロボットの「脳の部分」を扱っている。（脳だけ、というわけではない）

* ロボットの研究者は[人工知能](https://d.hatena.ne.jp/keyword/%E4%BA%BA%E5%B7%A5%E7%9F%A5%E8%83%BD)の研究者というわけではない。

## 歴史

### ✅ 💻ENIAC

* 1946年、アメリカ ペンシルバニア大学。

* 世界初の汎用電子式コンピュータ。

    ![](https://upload.wikimedia.org/wikipedia/commons/6/6c/ENIAC_Penn1.jpg)

### ✅ ダートマス会議

* 1956年、アメリカで開催。

* 🎩ジョン・マッカーシーが初めて「人工知能（AI）」という言葉を使った。

* 世界初の人工知能プログラムといわれる💻[ロジック・セオリスト](https://ja.wikipedia.org/wiki/Logic_Theorist)のデモを実施した。

    ![](https://miro.medium.com/max/1000/0*8MW8iP2QC_WNhmiW)

### ✅ 第１次AIブーム

* **推論・探索** が中心。

* **トイ・プロブレム（おもちゃの問題）** は解けても、現実の問題は解けないことが判明。

    ⇒ 失望へ

### ✅ 第２次AIブーム

* 💻**エキスパートシステム**が流行し、[ナレッジエンジニア](https://www.weblio.jp/content/%E3%83%8A%E3%83%AC%E3%83%83%E3%82%B8%E3%82%A8%E3%83%B3%E3%82%B8%E3%83%8B%E3%82%A2)が必要とされた。

    ```
    ナレッジエンジニアとは、
    人工知能（AI）を応用したシステム構築を専門とする技術者（エンジニア）のことである。（引用）
    ```

* 日本

    💻**第五世代コンピュータ**という大型プロジェクトを推進、エキスパートシステム等に取り組んだ。

* 知識の蓄積・管理は大変！ということに気づく。

    ```
    第五世代コンピュータとは、
    通商産業省（現経済産業省）が1982年に立ち上げた国家プロジェクトの開発目標である。（引用）
    ```

    ⇒ 失望へ

### ✅ 第３次AIブーム

* **機械学習・特徴表現**が中心。

* ビッグデータによる**機械学習**、特徴量による**ディープラーニング（深層学習）** が流行。

# ２．📘人工知能をめぐる動向

# 2-1.📘探索・推論

## 探索・推論の手法

### ✅ 探索木

* 幅優先探索

    最短距離の解が必ずわかる。すべてを記憶するためメモリ容量が必要。

    ![](https://upload.wikimedia.org/wikipedia/commons/b/bc/Breadth-first-tree.png)

* 深さ優先探索

    メモリは少なめでよいが、最短距離が必ずわかるわけではない。

    ![](https://upload.wikimedia.org/wikipedia/commons/5/5d/Depth-first-tree.png)

### ✅ [ハノイの塔](https://ja.wikipedia.org/wiki/%E3%83%8F%E3%83%8E%E3%82%A4%E3%81%AE%E5%A1%94)

* 以下のルールに従ってすべての円盤を右端の杭に移動させられれば完成。

    * 3本の杭と、中央に穴の開いた大きさの異なる複数の円盤から構成される。

    * 最初はすべての円盤が左端の杭に小さいものが上になるように順に積み重ねられている。

    * 円盤を一回に一枚ずつどれかの杭に移動させることができるが、小さな円盤の上に大きな円盤を乗せることはできない。

    ![](https://upload.wikimedia.org/wikipedia/commons/6/60/Tower_of_Hanoi_4.gif)

### ✅ ロボットの行動計画

* [プランニング](https://ja.wikipedia.org/wiki/%E8%87%AA%E5%8B%95%E8%A8%88%E7%94%BB)

    * オフラインプランニング・静的プランニング

        周囲の状況が既知で、その構造がよく理解されている場合に、行動の計画や戦略をあらかじめ組み立てて(計算して）おくこと。

    * オンラインプランニング・動的プランニング

        未知の環境において、周囲の状況が明らかになるにつれて行動の計画や戦略を修正すること。

    * リプランニング

        計画・戦略を修正すること。

* STRIPS

    * Stanford Research Institute Problem Solver

    * 自動計画に関する人工知能の一種。

    * 前提条件、行動、結果を記述する。

* SHRDLU

    * 1970年、スタンフォード大学、🎩テリー・ウィノグラード。

    * 英語の指示により画面上の積み木を動かす。

    * 成果はCycプロジェクトに引き継がれている。

    * 【Youtube】SHRDLU in Action

        [![](https://img.youtube.com/vi/bo4RvYJYOzI/0.jpg)](https://youtu.be/bo4RvYJYOzI "SHRDLU in Action")
    
### ✅ ボードゲーム

* 1996年、💻**IBM DeepBlue（ディープブルー）。「[力任せの探索](https://ja.wikipedia.org/wiki/%E5%8A%9B%E3%81%BE%E3%81%8B%E3%81%9B%E6%8E%A2%E7%B4%A2)」だったが、チェスの世界チャンピオンを破った。

    ```
    力まかせ探索（Brute-force search）またはしらみつぶし探索（Exhaustive search）は、
    単純だが非常に汎用的な計算機科学の問題解決法であり、
    全ての可能性のある解の候補を体系的に数えあげ、
    それぞれの解候補が問題の解となるかをチェックする方法である。（引用）
    ```

* 2012年、💻ボンクラーズ が将棋において永世棋聖に勝利。

* 2013年、💻ponanza が将棋において現役プロ棋士に勝利。

* 2016年、💻AlphaGo（アルファ碁）が韓国のプロ棋士に勝利。

    ディープラーニングが使われた。

    ![](https://tech-camp.in/note/wp-content/uploads/alphago-1024x519.png)

* 2017年、💻elmo が世界コンピュータ将棋選手権において ponanza に勝利。elmo 同士の対戦を行うことで学習を行った。

### ✅ コスト

* 効率よく探索するため、時間や費用といったコストの概念を取り入れている。

* ヒューリスティックな知識を利用して探索を短縮することができる。

    ※ヒューリスティック：経験則の、試行錯誤的な

### ✅ [Mini-Max法](https://ja.wikipedia.org/wiki/%E3%83%9F%E3%83%8B%E3%83%9E%E3%83%83%E3%82%AF%E3%82%B9%E6%B3%95)

* ゲーム戦略で利用される。

* 想定される「最大の損害」が最小になるように決断を行う戦略のこと。

* 自分の番はスコア最大、相手の番はスコア最小になるような戦略をとる。

* この手法は全探索を行うため効率が悪い。

### ✅ [α-β法](https://ja.wikipedia.org/wiki/%E3%82%A2%E3%83%AB%E3%83%95%E3%82%A1%E3%83%BB%E3%83%99%E3%83%BC%E3%82%BF%E6%B3%95)

* Mini-Max法の応用アルゴリズム。

* 読む必要のない手を打ち切ることで高速化を図っている。

* αカットは関心範囲の最小値のカット、βカットは最大値のカットを行う。

### ✅ モンテカルロ法

* 特徴：

    プレイアウト（ゲームを一度終局までもっていく）の結果、どの方法が一番勝率が高いかを評価する。

* デメリット：

    ブルートフォース（力任せな方法）のため、組合せが多いと計算しきれない。

# 2-2.📘知識表現

## 知識表現

### ✅ 💻ELIZA（イライザ）

* 1966年、🎩ジョセフ・ワイゼンバウム。

* 「人工無能」の元祖。精神科セラピストを演じた。

* パターンに合致したら返答する「ルールベース」である。

* イライザ効果：あたかも本物の人間と話しているように錯覚すること。

* その後開発された💻PARRYと会話した記録が残されており（RFC439）、中でもICCC1972が有名。

    ![](https://upload.wikimedia.org/wikipedia/commons/4/4e/ELIZA_conversation.jpg)

## ✅ エキスパートシステム

### ✓ 💻DENDRAL

* 1960年代、スタンフォード大学　🎩エドワード・ファイゲンバウム。

* 未知の有機化合物を特定する。質量分析法で分析する。

    ```
    質量分析法とは、
    分子をイオン化し、そのm/zを測定することによってイオンや分子の質量を測定する分析法である。（引用）。
    ```

### ✓ 💻マイシン（MYCIN）

* 1970年、スタンフォード大学。

* ルールベースで血液中のバクテリアの診断支援を行った。

* 正解確率の高い細菌名のリスト、信頼度、推論理由、推奨される薬物療法コースを示した。

* 精度は専門医の80%に対し、69%であった。

## ✅ 意味ネットワーク

* semantic network

* 人間の記憶の一種である意味記憶の構造を表すためのモデル。

* 単語同士の意味関係をネットワークによって表現する。

* 概念を表す節（ノード）と、概念の意味関係を表す辺（エッジ）からなる、有向グラフまたは無向グラフである。

    ```
    無向グラフのエッジには方向性がありません。
    エッジは "双方向" の関係を示します。 
    有向グラフのエッジには方向性があります。エッジは "一方向" の関係を示します。（引用）
    ```

    ![](https://upload.wikimedia.org/wikipedia/commons/thumb/6/67/Semantic_Net.svg/305px-Semantic_Net.svg.png)


## ✅ [Cycプロジェクト](https://www.cyc.com/)

* 1984年、🎩ダグラス・レナート。

* すべての一般知識を取り込もうという活動。

* 2001年からはOpenCycとして公開されている。

## [オントロジー（ontology）](https://ja.wikipedia.org/wiki/%E3%82%AA%E3%83%B3%E3%83%88%E3%83%AD%E3%82%B8%E3%83%BC_(%E6%83%85%E5%A0%B1%E7%A7%91%E5%AD%A6))

* 🎩トム・グルーバーが提唱。

* 知識を体系化する方法論で、「概念化の明示的な仕様」（知識を記述するための仕様）と定義されている。

* 知識の形式表現であり、あるドメインにおける概念間の関係のセットである。

* is-a 関係（上位概念、下位概念、推移律）、part-of 関係を用いる。

### ✅ [セマンティックウェブ](https://ja.wikipedia.org/wiki/%E3%82%BB%E3%83%9E%E3%83%B3%E3%83%86%E3%82%A3%E3%83%83%E3%82%AF%E3%83%BB%E3%82%A6%E3%82%A7%E3%83%96)

* [W3C](https://d.hatena.ne.jp/keyword/W3C) の🎩ティム・バーナーズ＝リーによって提唱されたプロジェクト。

* ウェブページの意味を扱うことができる「標準」や「ツール群」の開発により、ワールド・ワイド・ウェブの利便性を向上させようというもので、オントロジーを利用する。

* プロジェクトの目的は、ウェブページの閲覧という行為（データ交換）に対し、意味の疎通を付け加えることにある。

* 情報リソースに意味を付与することで、コンピュータで高度な意味処理を実現したり、文書の意味に即した処理が行えるようにする。

### ✅ ヘビーウェイトオントロジー（重量オントロジー）

* 人間が厳密にしっかりと考えて知識を記述していくアプローチ。

* 構成要素や意味的関係の正統性については、哲学的な考察が必要。

### ✅ ライトウェイトオントロジー（軽量オントロジー）

* コンピュータにデータを読み込ませ、自動で概念間の関係性を見つけるアプローチ。

* 完全に正でなくても使えればOKと考える。

* **ウェブマイニング**、**データマイニング**で利用される。

    ```
    ウェブマイニング（web mining）とは、
    ウェブサイトの構造やウェブ上のデータを利用して行うデータマイニングのことである。（引用）

    データマイニング（Data mining）とは、
    統計学、パターン認識、人工知能等のデータ解析の技法を大量のデータに網羅的に適用することで知識を取り出す技術のことである。（引用）
    ```

### ✅ 💻ワトソン

* IBM Watson

* Question-Answering（質問応答）型。

* 2011年、アメリカのクイズ番組である「[ジェパディ！](https://ja.wikipedia.org/wiki/%E3%82%B8%E3%82%A7%E3%83%91%E3%83%87%E3%82%A3!」で優勝した。

* ライトウェイトオントロジーに該当する。

    ![](https://assets.media-platform.com/bi/dist/images/2019/11/18/5dceff793afd373944079d27-w1280.png)

### ✅ 💻東ロボくん

* 2011年～2016年、国立情報学研究所。

* プロジェクトリーダーは🎩新井紀子。（著書『[AI vs.教科書が読めない子どもたち](https://www.amazon.co.jp/dp/4492762396』）

* 読解力に問題があり、何かしらのブレイクスルーが必要と判断され、開発は凍結された。

* その後、新井氏は人間側の読解力の問題に注目し、さまざまな活動を行っている。（[TED](https://www.ted.com/talks/noriko_arai_can_a_robot_pass_a_university_entrance_exam?language=ja)がわかりやすい）

# 2-3.📘機械学習

* ビッグデータを活用する。

* 統計的自然言語処理を行う。

* 応用例：

    レコメンデーションエンジン、スパムフィルター

## レコメンデーションシステム

* おすすめを提示するシステム。

### ✅ 協調ベースフィルタリング

* ユーザーの購買履歴からおすすめを表示するアプローチ。

* ユーザーの行動をもとにレコメンドする。

### ✅ 内容ベースフィルタリング

* アイテムの特徴をもとにおすすめを表示するアプローチ。

* 検索キーワードに関連する類似アイテムをレコメンドする。

* アイテムの特長ベクトルをもとにレコメンドである。

# 2-4.📘深層学習（ディープラーニング）

## 深層学習　関連手法

### ✅ 単純パーセプトロン

* シンプルなニューラルネットワーク。

* ステップ関数で表現できるがニューラルネットワークでは利用できない。

    ![](https://upload.wikimedia.org/wikipedia/commons/thumb/d/d9/Dirac_distribution_CDF.svg/1920px-Dirac_distribution_CDF.svg.png)

### ✅ ディープラーニング

* ニューラルネットワークを多層にしたもの。

### ✅ バックプロパゲーション

* 誤差逆伝播学習法

* ニューラルネットワークの学習におけるアルゴリズム。

### ✅ 自己符号化器（オートエンコーダ）

* 入力したものと同じものを出力して学習する。

## 深層学習　実装例

### ✅ 💻SuperVision

* 2012年、トロント大学、🎩ジェフリー・ヒントン。

* ILSVRC（Imagenet Large Scale Visual Recognition Challenge）2012 で勝利した。

* エラー率は26%台から15.3%へ劇的に改善。

* その後、2015年に人間の認識率（約5.1%）を抜いた。

* AlexNet（畳み込みニューラルネットワーク、CNN）を採用。

* 前年度まではサポートベクターマシンが主流だったが、ここからCNNに切り替わったことになる。

# ３．📘人工知能分野の問題

# 3-1.📘トイプロブレム（おもちゃの問題）

* ルールが決まっている問題（迷路、オセロなど）は解けても、現実世界に存在する複雑な問題は解けないという問題。

# 3-2.📘[フレーム問題](https://ja.wikipedia.org/wiki/%E3%83%95%E3%83%AC%E3%83%BC%E3%83%A0%E5%95%8F%E9%A1%8C)

* 1969年、🎩ジョン・マッカーシーと🎩パトリック・ヘイズが提唱。

* 人工知能における重要な難問の一つ。

* 有限の情報処理能力しかないロボットには、現実に起こりうる問題全てに対処することができない。

* 🎩ダニエル・デネット：

    考えすぎて何も解決できないロボットを例示し、フレーム問題の難しさを伝えた。

# 3-3.📘強いAI・弱いAI

* 🎩ジョン・サールが提唱。

* 強いAI：

    人間のような心、自意識を持つAI。

* 弱いAI：

    便利な道具であればよいという考え方によるAI。

## 汎用AI、特化型AI

### ✅ 汎用AI

* フレーム問題を打ち破るAIのことで、人間のように様々な課題に対処することができる。

### ✅ 特化型AI

* フレーム問題を打ち破っていないAIのこと。

## 強いAIに関する主張

### ✅ 中国語の部屋

* 🎩ジョン・サールが論文で発表した。

* 強いAIは実現不可能だという思考実験。

    ```
    中国語を理解できない人を小部屋に閉じ込めて、
    マニュアルに従った作業をさせるという内容。
    チューリング・テストを発展させた思考実験で、意識の問題を考えるのに使われる。（引用）
    ```

### ✅ 🎩ロジャー・ペンローズ

* イギリス生まれの数学者、宇宙物理学・理論物理学者。

* 「量子効果が絡んでいるため強いAIは実現できない」と主張した。

# 3-4.📘身体性

* 知能の成立には身体が不可欠であるという考え方。

* 物理的な身体により外部環境との相互作用を行うことができる。

* しかし、GoogleやFacebookの研究スピードでは、身体性の研究をすっ飛ばして概念獲得や意味理解ができてしまう可能性もある。

# 3-5.📘シンボルグラウンディング問題

* 🎩スティーブン・ハルナッド。

* 記号（シンボル）と現実世界の意味はどのようにして結びつけられるのかという問題。

* 外部世界を内部化（記号化、シンボル化）した時点で、外界との設置（クラウディング）が切れてしまうという問題。

### ✅ 知識獲得のボトルネック

* 人間が持っている知識は膨大であり、それらを獲得することは困難である。

* 特にエキスパートシステムの開発において問題となった。

### ✅ ニューラル機械翻訳

* NMT、Neural Machine Translation

* ニューラルネットワーク、ディープラーニングを利用した機械翻訳。

* 日本語の翻訳品質を飛躍的に高めた。

* 従来の方式には**ルールベース機械翻訳（RMT）**、**統計的機械翻訳（SMT）**がある。

    ```
    「ルールベース機械翻訳（RMT：Rule Based Machine Translation）」は、
    登録済みのルールを適応することで原文を分析し、訳文を出力する機械翻訳の方法です。
    
    「統計的機械翻訳（SMT：Statistical Base Machine Translation）」は、
    コンピュータに学習用の対訳データを与え、統計モデルを学習させることで訳文を出力させる方法です。（引用）
    ```

### ✓ seq2seq

* 再帰型ニューラルネットワーク（RNN）を使った文の生成モデル。

* 時系列データを入力し、時系列データを出力する。

* 別の言語に置き換えたり（翻訳）、質問を回答に置き換えたり（質問・回答）できる。

# 3-6.📘特徴量設計

* モデルの性能は、注目すべきデータの特徴（特徴量）の選び方により決定づけられるが、それを人間が見つけ出すのは難しい。

* 機械学習自身に発見させるアプローチを特徴表現学習という。

# 3-7.📘[チューリングテスト](https://ja.wikipedia.org/wiki/%E3%83%81%E3%83%A5%E3%83%BC%E3%83%AA%E3%83%B3%E3%82%B0%E3%83%BB%E3%83%86%E3%82%B9%E3%83%88)

* 🎩アラン・チューリングにより考案された。

* ある機械が知的かどうか（人工知能であるかどうか）を判定するためのテスト。

### ✅ ローブナーコンテスト

* [ローブナー賞](https://ja.wikipedia.org/wiki/%E3%83%AD%E3%83%BC%E3%83%96%E3%83%8A%E3%83%BC%E8%B3%9E)

* [Official Page](https://aisb.org.uk/aisb-events/)

* チューリングテストの合格を目指すコンテスト。

# 3-8.📘シンギュラリティ（技術的特異点）

* 🎩レイ・カーツワイルの著書で提唱された。

    * 2029年：人工知能が人間よりも賢くなる

    * 2045年：シンギュラリティの到来（2045年問題ともいわれる）

* 「収穫加速の法則」により「強いAI」が実現され、人間には予測不可能な変化が起こるとされている。

### ✓ [収穫加速の法則](https://ja.wikipedia.org/wiki/%E5%8F%8E%E7%A9%AB%E5%8A%A0%E9%80%9F%E3%81%AE%E6%B3%95%E5%89%87)

* レイ・カーツワイルが提唱した経験則。

* 一つの重要な発明が他の発明と結び付くことで、次の重要な発明の登場までの期間を短縮する。これによりイノベーションの速度が加速され、科学技術は直線的ではなく指数関数的に進歩するというもの。

### ✓ シンギュラリティに関する発言等

* 🎩レイ・カーツワイル	
    
    シンギュラリティは2045年に到来する

* 🎩ヒューゴ・デ・ガリス

    シンギュラリティは21世紀の後半に来る

* 🎩オレン・エツィオーニ

    シンギュラリティの終末論的構想は馬鹿げている

* 🎩ヴィーナー・ヴィンジ

    機械が人間の役に立つふりをしなくなること

* 🎩スティーブン・ホーキング

    AIの完成は人類の終焉を意味するかもしれない

* 🎩イーロン・マスク

    危機感を持ち非営利のAI研究組織 OpenAI を設立。

    OpenAI Gym（強化学習のシミュレーション環境）を発表。

# ４．📘機械学習の具体的手法

# 4-1.📘代表的な手法

## ✅ 教師あり学習

### ✓ 回帰問題

* [線形回帰 (linear regression)](https://ja.wikipedia.org/wiki/%E7%B7%9A%E5%BD%A2%E5%9B%9E%E5%B8%B0)

    ![](https://upload.wikimedia.org/wikipedia/commons/b/be/Normdist_regression.png)

    * 統計学における回帰分析の一種。

    * 特徴：通常の線形回帰は過学習を起こしやすい。ラッソ回帰やリッジ回帰で過学習を抑制する。

* 回帰分析の種類

    * 単回帰分析 ： ひとつの説明変数により、ひとつの目的変数を予測する。
    
    * 重回帰分析 ： 複数の説明変数から、ひとつの目的変数を予測する。

        + 多重共線性 ： 説明変数の選択において、相関係数の絶対値が最大値に近い特徴量のペアを選ぶと、予測の精度が悪化する性質。

+ [ラッソ回帰 (lasso regression)](https://ja.wikipedia.org/wiki/%E3%83%A9%E3%83%83%E3%82%BD%E5%9B%9E%E5%B8%B0)

    * 直線回帰に正則化項（L1ノルム）を加えた回帰分析。

* リッジ回帰 (ridge regression)

    * 直線回帰に正則化項（L2ノルム）を加えた回帰分析。

* 参考資料

    * ノルム ： いろいろなものの「大きさ」を表す。

    * [多重共線性とは？ 〜 概要と対応方法 〜](https://xica.net/vno4ul5p/)

### ✓ 分類問題

* ロジスティック回帰 (logistic regression)

    * 活性化関数として シグモイド関数 を使い、重回帰分析により二値分類を行う。

        * シグモイド関数は 対数オッズ（ロジット） の逆関数である。（ロジット変換（正規化））

        * 対数をとる前の オッズ とは、ある事象が起こる確率 p と起こらない確率 1−p の比のこと。

        * 最小化を行う関数として **尤度関数** が用いられる。

* 多クラスロジスティック回帰

    * 活性化関数に **ソフトマックス関数** を利用し、多クラス分類を行う。

* ランダムフォレスト (random forest)

    * ブートストラップサンプリングにより、アンサンブル学習を行う。

    * バギングに該当する。

    * バギング

        * データ全体からランダムに一部データを用いて、複数のモデルを作る（学習する）方法。並列処理になる。

    * ブースティング (boosting)

        * 一部のデータを繰り返し抽出し、複数のモデルを学習させる。

        * 逐次処理のため、ランダムフォレストより時間がかかる。

        * AdaBoost、勾配ブースティング、XgBoost

* サポートベクターマシン

    * コンセプトはマージンの最大化を行うこと。

    * スラック変数
    
        * 誤分類を許容する工夫をする、線形分離不可能なデータのマージンを最大化する。

    * カーネル法

        * カーネル関数により高次元への写像を行い線形分離可能にする。

            [![](https://miro.medium.com/max/2400/1*mCwnu5kXot6buL7jeIafqQ.png)](https://medium.com/@zxr.nju/what-is-the-kernel-trick-why-is-it-important-98a98db0961d)


    * カーネルトリック

        * 写像の際に計算が複雑にならないように式変形するテクニック。計算量を大幅に削減する。

* ニューラルネットワーク

    * キーワード：

        ニューロン、神経回路、単純パーセプトロン、多層パーセプトロン、入力層、出力層、重み、隠れ層、活性化関数、シグモイド関数、誤差逆伝播法

    * ロジスティック回帰はニューラルネットワークの一種。（単純パーセプトロンと同等）

## ✅ 教師なし学習

* k-means法

    * クラスタリングの手法。

        * 課題：クラスタリングを行う処理の初期値の取り方により結果が異なる。（偏りが生じる）

        * kNN法はクラス分類（教師あり学習）の手法なので注意！

* k-means++

    * k-meansの課題である初期値の取り方を工夫することにより、結果に偏りが生じることを抑制する。

* 主成分分析（PCA）

    * **次元削減**の手法のひとつ。

    * 相関のある多数の変数から、特徴をよく表している成分（主成分）を特定し要約することで次元を削減する。

    * 寄与率：各成分の重要度を表す。

## ✅ 強化学習

* エージェントの目的は収益（報酬・累積報酬）を最大化する方策を獲得すること。

* エージェントが行動を選択することで状態が変化し、最良の行動を選択する行為を繰り返す。

* 状態 →（方策により）行動 → 収益を獲得 → 次の状態 →・・・

# 4-2.📘データの扱い

* ホールドアウト検証

    * データを訓練データとテストデータに分割（例えば７：３）し利用する。

    * 訓練データでの学習によりモデルを構築し、テストデータで検証を行う。

    * 交差検証の一種と説明されることもあるが、データを交差させないため交差検証ではない。

* k-分割交差検証

    * テストデータをk個に分割し、ひとつをテストデータ、その他を訓練データとする。

    * テストデータを順次入れ替えることで、少ないテストデータでもより安定したモデルを選択できる。

* 訓練データ、検証データ、テストデータ

    * 訓練データによる学習でモデルを作成する。

    * 検証データによりハイパーパラメータ等を調整する。

    * テストデータにより評価を行う。

## ✅ 欠損値処理

### ✓ リストワイズ法

* 欠損があるサンプルをそのまま削除する方法。

* 欠損に偏りがある場合はデータの傾向を変えてしまうので注意が必要。

### ✓ 回帰補完

* 欠損しているある特徴量と相関が強い他の特徴量が存在している場合に有効。

## ✅ カテゴリデータ

### ✓ マッピング

* 順序を持つデータの場合、数値の辞書型データにマッピングする。

### ✓ ワンホットエンコーディング

* 順序を持たないデータの場合、各カテゴリごとにダミー変数を割り当てる。

# 4-3.📘応用・評価指標

## ✅ 混同行列

* 正解率 ： 陽性・陰性を含めた全正解数に対する、予測での正解数。全体の精度を上げたい場合の評価項目。

* 適合率 ： 予測での陽性数に対する、実際の陽性数（陽性だ！と思ったものがどれくらい合っているか）。偽陽性を削減したい場合の評価項目。

* 再現率 ： 実際の全陽性に対する、予測での陽性正解数（すべての陽性に対し、予測でどれくらい陽性が再現できているか）。偽陰性を削減したい場合の評価項目。適合率と再現率は相反関係にある。

* F値 ： 適合率と再現率の調和平均。

## ✅ オーバーフィッティング、アンダーフィッティング

* オーバーフィッティング

    * 訓練データに適合しすぎており（過学習）、テストデータの精度が低下している状態。（汎化性能が低い状態）

    * 訓練データにフィットしすぎないように、正則化項の導入などを行ったのち、改めて学習を行う必要がある。

* アンダーフィッティング

    * 訓練が不十分で、訓練データ・テストデータの両方に対して精度が低い状態。

    * 学習をさらに進めることで改善することがある。

## ✅ 正則化

* 訓練誤差ではなく、汎化誤差を小さくする（汎化性能を高める）ための手法。正則化項を導入することでオーバーフィッティグを防止する。

* L1正則化：ラッソ正則化（Lasso Normalization）。不要なパラメータを削減できる（ゼロにする）。この特徴をスパース性という。

* L2正則化：リッジ回帰（Ridge Normalization）。Lassoと違い特徴量の選択は行わないが、パラメータのノルムを小さく抑えることができる（パラメータのノルムにペナルティを課す）。重み減衰（Weight Decay）ともいう。

* Elastic Net：L1正則化、L2正則化を組み合わせたもの。

* 参考：[Qiita-【機械学習】ラッソ回帰・リッジ回帰について　メモ](https://qiita.com/nanairoGlasses/items/57515340a1bc24ffe445#%E3%83%A9%E3%83%83%E3%82%BD%E5%9B%9E%E5%B8%B0%E3%81%A8%E3%81%AF)

# ５．📘ディープラーニングの概要

# 5-1.📘ニューラルネットワークとディープラーニング

## ✅ 単純パーセプトロン

* 線形分類しかできない。

    ![](https://miro.medium.com/max/645/0*LJBO8UbtzK_SKMog)

## ✅ 多層パーセプトロン

* 多層化することで、非線形分類が出来るようになった。

    ![](https://miro.medium.com/max/700/1*-IPQlOd46dlsutIbUq1Zcw.png)

## ✅ ディープラーニング

* 概念としては1960年代には既に存在していた。

* ディープニューラルネットワークを用いたもので、ニューロンをいくつもつなげており、複雑な関数を近似できる。

* 検証方法として、通常はデータ量が多いため、ホールドアウト検証でよい（十分である）。

* 問題：

    * オーバーフィッティング（過学習）しやすい。（但し、精度に特別バラつきが出やすいというわけではない。）

    * 勾配消失問題を起こしやすい。

    * 事前に調整すべきパラメータ数が非常に多い。

## ✅ 用語

* バッチサイズ

    * イテレーションで用いるデータセットのサンプル数。

    * 全データをバッチサイズで切り分ける。

* イテレーション

    * 重みの更新を行う回数。

    * データセットに含まれるデータが少なくとも１回は実行されるようにする学習回数。

    * データセットとバッチサイズが決まれば、イテレーションは自動的に決まる。

        * イテレーション＝データセット／バッチサイズ

* エポック

    * 訓練データを学習に用いた回数。

    * データセットのバッチサイズ分割からイテレーション実行までの処理を繰り返す回数のこと。

    * エポック数が２の場合、データセットを２回使うことになる。（重みの更新回数＝イテレーション数×バッチ数×２）

* 参考

    * [機械学習／ディープラーニングにおけるバッチサイズ、イテレーション数、エポック数の決め方 - Qiita](https://qiita.com/kenta1984/items/bad75a37d552510e4682)

        ![](https://miro.medium.com/max/700/1*AOiD8LEDWrWy5l_f9qgweQ@2x.jpeg)

# 5-2.📘既存のニューラルネットワークにおける問題

* 課題

    * 隠れ層の層数を増やすと、誤差逆伝播時に誤差が最後（入力層付近）まで正しく反映されない。

* 原因

    * シグモイド関数の導関数（微分式）の最大値は0.25のため、層を進ごとに伝播させる誤差の値がどんどんと小さくなってしまう。（**勾配消失問題**）

        * 0.25×0.25×0.25… と計算していくと値が小さくなっていく。

# 5-3.📘ディープラーニングのアプローチ

## ✅ オートエンコーダ（autoencoder、自己符号化器）

* 🎩ジェフリー・ヒントンが提唱。

* 入力と出力が同じになるニューラルネットワーク。（＝正解ラベルが入力と同じ）

* 次元削減が行える。

    ![](https://upload.wikimedia.org/wikipedia/commons/3/34/%D0%90%D0%B2%D1%82%D0%BE%D1%8D%D0%BD%D0%BA%D0%BE%D0%B4%D0%B5%D1%80.png)

## ✅ 積層オートエンコーダ

* オートエンコーダを積み重ねて、逐次的に学習させる（事前学習）ことで重みを調整する

## ✅ ファインチューニング

* 積層オートエンコーダにロジスティック回帰層（あるいは線形回帰層）を追加し、仕上げの学習を行う。

## ✅ [深層信念ネットワーク](https://www.kyoritsu-pub.co.jp/ai/pdf/7-9deeplearning.pdf)

* 🎩ジェフリー・ヒントンが提唱。

* 確定的モデルに分類される。（深層ボルツマンマシンは確率的モデルに分類される）

* 隠れ層の数が多いニューラルネットワークで、効率の良い近似学習手法を提案した。

* 具体的には、教師なし学習による事前学習（制限付きボルツマンマシン）により効率的な学習を実現。

* ※ボルツマンマシン（[参考](https://www.sist.ac.jp/~kanakubo/research/neuro/boltzmannmachine.html)）

    ヒントンらによって開発された、確率的に動作するニューラルネットワーク。ネットワークの動作に温度の概念を取り入れ、最初は激しく徐々に穏やかに動作する（擬似焼きなまし法）ように工夫している。

## ✅ 現状

* 事前学習は計算コストが非常に高いので今は使われておらず、活性化関数を工夫することで解決している。

# 5-4.📘CPU と GPU

## ✅ CPU と GPU

* CPU：Central Processing Unit、中央演算処理装置

    * （GPUと相対的に）少ないコア数で、複雑な計算を直列処理する。

    * コア単位の性能は高い。

* GPU：Graphics Processing Unit

    * リアルタイム画像処理に特化した演算装置。近年は機械学習などでも利用される。

    * 多くのコアで、単純な計算を並列処理する。

    * コア単位の性能は低い。

* GPGPU：General-Purpose computing on GPU。

    * 画像以外の目的に最適化されたGPU。

* 開発メーカー

    * [NVIDIA](https://ja.wikipedia.org/wiki/NVIDIA)
    
        * GPUの開発をリード。ディープラーニングには不可欠。

        * 2006年、GUGPUの開発基盤であるCUDAを発表。

        * 2020年、Arm（英国のコンピューティング・アーキテクチャ開発企業）を買収。

        * 2020年現在、[GPUマーケットシェアの8割程度を占める](https://www.techradar.com/news/nvidia-crushes-amd-with-80-gpu-market-share-ahead-of-ampere-launch)。

    * [AMD](https://ja.wikipedia.org/wiki/%E3%82%A2%E3%83%89%E3%83%90%E3%83%B3%E3%82%B9%E3%83%88%E3%83%BB%E3%83%9E%E3%82%A4%E3%82%AF%E3%83%AD%E3%83%BB%E3%83%87%E3%83%90%E3%82%A4%E3%82%BB%E3%82%BA)

        * ATI Technologiesを吸収合併し、GPUの開発にあたってきた。

        * 2020年現在、[GPUマーケットシェアの2割程度](https://www.techradar.com/news/nvidia-crushes-amd-with-80-gpu-market-share-ahead-of-ampere-launch)である。

    * Google

        * TPU：Tensor Processing Unit、テンソル計算に最適化されたもの。

        * 分散並列技術である[Dist Belief](https://qiita.com/To_Murakami/items/d470e9dc98ac92cf299d)もGoogleにより提案されたものである。

# 5-5.📘ディープラーニングにおけるデータ量

## ✅ バーニーおじさんのルール (Uncle Bernie's rule)

* モデルを構築するためには、パラメータ数の10倍のデータ数が必要であるという経験則。

* バーニーおじさんは、スタンフォード大学の教授である。

## ✅ 次元の呪い

* データの次元が増えることにより、様々な不都合が生じる法則のこと。

* 具体的には、データの次元（例えば特徴の数）が大きくなると、データ分析における計算量が指数関数的に増大してしまう問題のことを指す。

* 対策のためには、次元数を減らす必要がある。

## ✅ その他の機械学習に関する定理

### ✓ [ノーフリーランチ定理](https://ja.wikipedia.org/wiki/%E3%83%8E%E3%83%BC%E3%83%95%E3%83%AA%E3%83%BC%E3%83%A9%E3%83%B3%E3%83%81%E5%AE%9A%E7%90%86)

* 組み合わせ最適化の領域での定理。

    ```
    コスト関数の極値を探索するあらゆるアルゴリズムは、
    全ての可能なコスト関数に適用した結果を平均すると同じ性能となる
    ```

* あらゆる問題に対して万能なアルゴリズムは存在しない、ということ。

### ✓ みにくいアヒルの子定理

* 機械学習で、「普通のアヒル」と「みにくいアヒル」を見分けることはできない。

* 「認知できる全ての客観的な特徴」に基づくと、全ての対象は「同程度に類似」している。客観的に見ればどれも同じということ。
区別するためには、何らかの前提知識によって特徴量に重要性をつけなければならない。

### ✓ [モラベックのパラドックス](https://ja.wikipedia.org/wiki/%E3%83%A2%E3%83%A9%E3%83%99%E3%83%83%E3%82%AF%E3%81%AE%E3%83%91%E3%83%A9%E3%83%89%E3%83%83%E3%82%AF%E3%82%B9)

* 機械にとっては、高度な推論より１歳児レベルの知能・運動スキルを身に着ける方が難しい。

    ```
    高度な推論よりも感覚運動スキルの方が多くの計算資源を要する
    ```

# ディープラーニング　その他

## ディープラーニングのフレームワーク

* Tensorflow

    * Google社が開発。OSS。

    * 数値解析、機械学習、ニューラルネットワークに対応。

    * プログラムによりネットワークを記述する。

    * ※設定ファイルによりネットワークを記述　：　Caffe, CNTK

* Keras

    * Google社が開発。OSS。

    * Tensorflow上で動作し、ディープニューラルネットワークに対応。

    * プログラムによりネットワークを記述する。

* Chainer

    * Preferred Networks が開発。

    * Define-by-Runという形式を採用しており、データを流しながらニューラルネットワークを構築する。

    * ※構築後の実行はDefine-and-Runといわれる。

    * ※プログラムによりネットワークを記述する。

    * 2019年12月、開発を終了しPyTorchに移行すると発表。

* [PyTorch](https://ja.wikipedia.org/wiki/PyTorch)

    * Facebookが開発。

    * Chainerから派生。

## 🎩ヨシュア・ベンジオ

* ディープラーニングの父のひとりといわれる。

* 人間の知識では気づくことが出来ない共通点のことを「良い表現」としている。

    * 複数の説明変数の存在

    * 時間的空間的一貫性

    * スパース性

* ディープラーニングのアプローチとして以下に着目している。

    * 説明変数の階層的構造

    * タスク間の共通要因

    * 要因の依存の単純性

# ６．📘ディープラーニングの手法

# 6-1.📘活性化関数

* ニューラルネットワークにおいて、入力値から出力値を決定するための関数。

* 出力層は、シグモイド関数またはソフトマックス関数で確率を表現する必要がある。

    * ソフトマックス関数：各ユニットの出力の総和を１に正規化する機能がある

* 隠れ層ではさまざまな工夫ができ、tanh関数やReLU関数などが使われる

## ✅ シグモイド関数

* 値の範囲は 0～1 。

* 微分の最大値は 0.25 。

    * 層が深くなる（つまり「0.25×0.25×・・・」と積算を繰り返していく）と値がどんどんと小さくなるため、勾配を表す値も小さくなり消失してしまうという課題がある。

    ![](https://imagingsolution.net/wordpress/wp-content/uploads/2017/06/derivative-of-sigmoid-function_1.png)


## ✅ tanh関数（ハイパボリックタンジェント関数）：双曲線正接関数

* 値の範囲は -1～1 。（シグモイド関数よりも広い）

* 微分の最大値は 1 。

    * 値が大きいため、シグモイド関数よりも勾配が消失しづらいという特徴を持つ。

## ✅ ReLU関数（Rectified Linear Unit）：正規化線形関数

* 特徴
    
    * 現在、最もよく使われている。ニューラルネットワークにおける活性化関数のデファクトスタンダード。

    * x値が1より大きい場合、微分値が1になるため、勾配消失しにくい。

    * x値が0以下の場合、微分値が0になるため、学習がうまくいかない場合もある。

* 派生関数

    * Leaky ReLU： 0以下にわずかな傾き（0.01）を持たせることで微分値0を回避。

    * Parametric ReLU：0以下の傾きを固定値とせず、学習の対象としている。

    * Randomized ReLU：0以下の傾きをランダム値で設定する。

    * どれが一番よいと一概には言えない。

## ✅ 過学習

* 訓練誤差は小さいが、汎化誤差が大きい（テストデータに対する誤差が大きい）状態。

* 正則化項の導入やアーリーストッピングの適用など、過学習を抑制するためのさまざまな手法が研究されている。

# 6-2.📘学習率の最適化

* 学習は、損失関数を最小にするためのパラメータを探索することが目標。

* 微分（偏微分）により関数の最小値を求めたいが、多次元なので難しい。（計算量が多くなってしまう）

## ✅ 勾配降下法

* 微分値（傾き）を下っていくことでパラメータを最適化する。（パラメータごとに行う）

* 勾配降下法のハイパーパラメータ

    * イテレーション：計算の繰り返し数。

    * 学習率：勾配に沿って一度にどれくらい下る（移動するか）を表す。

        * 値が大きければ学習は大きく進むが、おかしなところに飛び出してしまう可能性もある。

        * 値が小さいと学習はなかなか進まない（計算量が増える）が、学習は着実に進む可能性が高い。

    * 手法の具体例

        * バッチ勾配降下法：すべての学習データを使って勾配降下を行う。

        * ミニバッチ勾配降下法：学習データから複数（バッチサイズ）を選択し誤差計算＆パラメータ更新を繰り返す。

        * 確率的勾配降下法（SGD）：学習データから確率的にデータを選択し、誤差計算＆パラメータ更新を行う。

## ✅ 勾配降下法の問題と改善

* 局所最適解にはまり、大域最適解が求められない場合がある。

    * 学習率の値を大きくすることで抜け出せるが、適宜値を小さくしていく必要がある。（値が大きいままだと飛び出して行ってしまう）

* ２次元の場合は停留点、３次元の場合は 鞍点（あんてん） にはまることもある。

    * はまった状態をプラトーという。次元が高いほど発生しやすい。
    
    * プラトーを抜け出す方法としてモーメンタム、AdaGrad、Adadelta、RMSprop、Adamなどがある。

    * 現在はRMSprop、Adamが利用される。([参考](https://postd.cc/optimizing-gradient-descent/))

### ✓ モーメンタム（Momentum、慣性）

* 以前に適用した勾配の方向を、現在のパラメータ更新にも影響させる。（慣性を効かせる）

* 勾配降下で進む方向が、大きくブレにくくなる。（図の青い線）

    ![](https://cdn-images-1.medium.com/max/842/0*TKxSMrG2xPLtcRVy.png)

### ✓ AdaGrad

* 学習率をパラメータに適応させることで自動的に学習率を調整する。（人が固定値を決めず、調整を機会に任せる）

* 稀なパラメータに対しては大きな更新、頻繁なパラメータに対しては小さな更新を行う。

* 具体的には、勾配を二乗した値を蓄積し、すでに大きく更新されたパラメータほど更新量（学習率）を小さくする。

* 課題　：　更新量が飽和したパラメータは更新されなくなる。

### ✓ Adadelta

* AdaGradの発展形

* 急速かつ単調な学習率の低下防止をはかったモデル。

### ✓ RMSprop

* AdaGradの発展形

* 急速かつ単調な学習率の低下防止をはかったモデル。

* Adadeltaと同時期に提唱されたもので、ほぼ同じの内容。

* 指数移動平均を蓄積することにより解決をはかったモデル。

### ✓ Adam

* それぞれのパラメータに対し学習率を計算し適応させるモデル。

* 勾配の平均と分散をオンラインで推定した値を利用する。

# 6-3.📘更なるテクニック

* 更に精度を高めるためのテクニックがさまざまある。

## ✅ ドロップアウト(Dropout)

* ランダムにニューロンをドロップアウトさせることで、ディープラーニングのオーバーフィッティング対策を行う。

* これにより、アンサンブル学習を行っているのと同じような状況になる。

## ✅ アーリーストッピング(early stopping)

* 学習を早めに打ち切ることで、ディープラーニングのオーバーフィッティング対策を行う。

* アンダーフィットからオーバーフィットに切り替わる途中で学習を止める、という単純なもの。

* どんな手法でも使えるため、非常に強力である。

## ✅ データの正規化・重みの初期化

### ✓ データの正規化

データの途中処理ではなく、始めの工夫も必要かつ有効である。

* 正規化（Normalization、≒Scaling）
    
    * データのスケールを合わせることで、学習時の収束を早める。

    * 一番簡単なのは各特徴量を ０～１ の範囲に変換（正規化）すること。

* 標準化

    * データ を 標準積分布（平均０、分散１） にする。

* 正則化（Regularization）

    * 過学習の回避を目的とする。

    * 損失関数に正則化項を追加することで、値の偏りを防止する。

* 【注意】

    * 上記３つは言葉と意味を混同しやすいので注意！

* 白色化

    * 各特徴量を無相関化したうえで標準化する、計算コストが高い

* 局所コントラスト正規化
    
    * 減算正規化と除算正規化の処理を行う。画像処理で利用される。

### ✓ 重みの初期化

* ディープニューラルネットワークでは伝播を経て分布が崩れるため、データの正規化手法が有効に働かない場合がある。

* 重みの初期値を工夫することで解決をはかることができる。

* 重み初期化の工夫として、乱数にネットワークの大きさを合わせた適当な係数をかけることにより、データ分布の崩れにくい初期値が考案されている。

    * Xavierの初期値：シグモイド関数、tanh関数で有効。

    * Heの初期値：ReLU関数で有効。

### ✓ ベイズ最適化

* ハイパーパラメータを含めた最適化問題とすることで、効率的なチューニングができる。

### ✓ スパースなデータ

* 疎なデータ。スパース性を用いて計算量を削減するといった工夫がなされる。

## ✅ バッチ正規化

* 各層に伝わってきたデータを、その層でまた正規化するアプローチ。（最初に正規化をするだけでなく、層ごとに正規化を繰り返す）

* データの正規化、重みの初期化と比較し、より直接的な手法となる。

* 非常に強力な手法で学習がうまくいきやすく、オーバーフィッティングしにくい。

* 学習が進むにつれて入力が変化する内部共変量シフトに対応することができる。（出力の分布の偏りを抑制する）

* 内部共変量シフト：

    入力の分布が学習の途中で大きく変わってしまう問題。

* 類似手法として、以下の正規化法がある
    
    * レイヤー正規化

    * インスタンス正規化

    * グループ正規化

    ![](https://i.imgur.com/kc2OPjA.png)

## ✅ End to End Learning（一気通貫学習）

* 入力から出力までを一括で行う、ディープラーニングにおける重要な方法論。

* 以前は処理を分割していた（せざるを得なかった）が、ディープラーニングにより一括処理ができるようになった。

# 6-4.📘CNN（畳み込みニューラルネットワーク）

* 特徴

    * 画像（２次元）をそのまま入力にできる。

    * 人間がもつ視覚野の神経細胞（単純型細胞 S細胞、複雑型細胞 C細胞）を模している。

    * 順伝播型ニューラルネットワークの一種で、時系列データの分析でも使える。

* ネオコグニトロン

    * 🎩福島邦彦が考案。

    * 上記を組み込んだ最初のモデルで多層構造になっている。

    * 学習方法は add-if silentであり 、微分（勾配計算）を用いない。

* LeNet

    * その後の1998年、🎩ヤン・ルカン（Facebookに招かれた研究者、「MNIST」の作成者）によって考案されたモデル。

    * ネオコグニトロンと基本的には同じ。

    * 畳み込み層 と プーリング層（サブサンプリング層） による複数組合せ構造。


    * 誤差逆伝播法 が使われる。

## ✅ 畳み込み層

* フィルタ（カーネル） により画像の特徴を抽出する操作。

* ストライド：フィルタを移動させる刻み。

* フィルタを通して特徴マップを得る、フィルタの各値が重みにあたる。

* 畳み込みは移動不変性の獲得に貢献、位置ずれの強いモデルが作れる。

* パラメータ数は全結合層よりも少ない。重み共有により有用な特徴量を画像の位置によって大きく変化させないためである。

## ✅ プーリング層

* 決められた演算を行うだけの層。（ダウンサンプリング、サブサンプリング）

* そのため、学習すべきパラメータはない。

### ✓ maxプーリング

* ２×２ごとに画像（特徴マップ）の最大値を抽出していく。

### ✓ avgプーリング

* 平均値をとる。平均プーリング。

### ✓ Lpプーリング

* 周りの値をp乗してその標準偏差をとる。

## ✅ 全結合層

* 分類のためには出力を１次元にする必要があ。全結合層によりデータをフラットにする。

* 最近の傾向：

    * 全結合層を用いない方法が増えており、１つの特徴マップに１つのクラスを対応させる Global Average Pooling がほとんどになっている。

## ✅ データ拡張

* 課題：

    * 同じ物体でも「明るさ」「角度」「大きさ」などにより見え方が異なる。

* 対応：

    * データ拡張（データの水増し）を行う。

        →　ずらす、反転、拡大・縮小、回転、歪め、切り取り、コントラスト変更 など

* 注意点：

    * データ拡張により意味の変わってしまう画像がある。（ex.いいねマークを逆さまにすると違う意味）

## ✅ CNNの発展形

* AlexNetの場合（基準として）

    （畳み込み＋プーリング)×３層 の構造をとる。

* VGG16（VGG・教師あり学習），GoogLeNet

* AlexNetよりも深いモデルになっている。

* 課題①

    * 層を深くすると計算が大変

* 工夫①

    * 小さなサイズの畳み込みフィルタにより次元（計算量）を削減する。

    * GoogLeNet　：　Inceptionモジュールというブロックを構成することで、並列計算を行いやすくする。

    * VGG16　：　2014年、GoogleNetに劣らない精度をたたき出した。オックスフォード大学による。

* 課題②

    * 超深層になると誤差の逆伝播がしづらくなるため、逆に性能が落ちる。

* 工夫②

    * Skip Connection　：　層を飛び越えた結合を加える。

    * ResNet

        Skip Connection を導入したモデル、伝播しやすくアンサンブル学習にもなる。

        入力層から出力層まで伝播する値と入力層の値を足し合わせたモデルである。
        
        入力層まで勾配値がきちんと伝わるようになり、1000 層といったかなり深い構造でも学習が可能となった。

        2015 年の ILSVRC では人間の成績を上回る成果をあげている。

    * ILSVRCのモデル推移

    [![](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/494282/19918227-7887-d697-f4e2-7602c64adbe9.png)](http://image-net.org/challenges/talks_2017/ILSVRC2017_overview.pdf)

# 6-5.📘RNN（リカレント ニューラルネットワーク）

* 特徴

    * 時間情報を反映できるモデル。隠れ層に時間情報（過去の情報）を持たせることができる。

    * 特徴は前回の中間層の状態を隠れ層に入力する再帰構造を取り入れたこと。

    * BackPropagation Through-Time(BPTT) ： 時間軸に沿って誤差を反映していく。

    * 自然言語処理でもよく用いられる。

    * 再帰型ニューラルネットワークで、閉路がある。

* 課題　（[参考1](https://qiita.com/t_Signull/items/21b82be280b46f467d1b)、[参考2](https://sagantaf.hatenablog.com/entry/2019/06/04/225239)）

    * 勾配消失問題

    * 入力重み衝突、出力重み衝突　：　重みが上下して精度が上がらない問題

    * ネットワークにループ構造が含まれるため、中間層が１層でも勾配消失問題が起こる。

* 解決策

    * LSTM手法を使う。

## ✅ LSTM（Long Short-Term Memory）

* 時系列データにおいてはデファクトスタンダード。Google翻訳でも利用されている。

* 🎩ユルゲン・シュミットフーバーと、ケプラー大学の🎩ゼップ・ホフレイターによる提案。

* 過去から未来に向けて学習し、遠い過去の情報でも出力に反映できる。

* 活性化関数の工夫ではなく、隠れ層の構造を変えることで解決する。

* LSTMブロック機構を適用

    * CEC（Constant Error Carousel）　：　誤差を内部にとどまらせ勾配消失を防ぐセル。

    * ゲート　：　入力、出力、忘却の３つ。

    * 各重み衝突に対応しつつ、誤差過剰を防止する忘却を持たせる。

* 機械翻訳や画像からのキャプション生成（画像の説明文生成）などにも利用できる。

* 課題

    * ゲートが多いため計算量が多い

## ✅ GRU（Gated Recurrent Unit）

* LSTMの計算量を少なくした手法。

* リセットゲート、更新ゲートからなる。

## ✅ RNNの発展形

### ✓ Bidirectional RNN

* 未来から過去方向にも学習できるモデル。

### ✓ RNN Encoder-Decoder

* 他モデルの問題

* 入力は時系列だが出力が一時点になってしまう。

* 特徴

    * 出力も時系列である（sequence-to sequence）。

    * モデルはエンコーダとデコーダからなる。

### ✓ Attention

* 他モデルの問題

* どの時点の情報がどれだけ影響力を持っているかまではわからない。

* 特徴
    
    * 時間の重みをネットワークに組み込んでいる。

* Attention GAN

    * 文章から画像を生成することができる。

# その他の応用

## ✅ 転移学習

* 学習済みのモデルを用いて追加学習を行う。

* 過学習を抑制することが出来る。

## ✅ 蒸留

* 学習済みの大規模モデルの入力と出力を使って新たに学習させる方法。

* 少ない計算資源で従来と同程度のモデルを作ることが出来る。

# 6-6.📘深層強化学習

## ✅ DQN（Deep Q-learning）

* 強化学習の手法であるQ学習と深層学習の組合せ。CNNの一種である。

* Q関数（＝行動価値関数）の最大化を目指す。

* DeepMind ブロック崩しで採用された。

    [![](https://img.youtube.com/vi/TmPfTpjtdgg/0.jpg)](https://youtu.be/TmPfTpjtdgg "SHRDLU in Action")

* 改良モデル：Double DQN, Dueling Network, Categorical DQN, Rainbow

* 応用事例：AlophaGo（アルファ碁）

# 6-7.📘深層生成モデル

ディープラーニングは生成タスクにも応用されている。

## ✅ 画像生成モデル

### ✓ VAE（Variable AutoEncoder）

* 変分オートエンコーダ、変分自己符号化器

* 変分ベイズ推定法の一種。

* 入力を統計分布に変換（平均と分散を表現）する。

* ランダムサンプリングしたものをデコードすると新しいデータが生成できる。

### ✓ GAN（敵対的生成ネットワーク）

* 🎩イアン・グッドフェローが提唱。

* ２種類のネットワーク（ジェネレータ：生成、ディスクリミネータ：識別）で競わせる。

* 画像生成への応用が顕著である。

* これ自体はモデルでなくアーキテクチャを指す。

* これを実装したモデルがDCGAN（Deep Convolutional GAN）。

* 🎩ヤン・ルカンは「機械学習において、この１０年で最もおもしろいアイデア」とコメント

# ７．📘ディープラーニングの研究分野

# 7-1.📘画像認識

## ✅ ILSVRC（Imagenet Large Scale Visual Recognition Challenge）

* 画像認識のコンペティション、課題は位置課題、検出課題の２つ。

* Imagenet

    * スタンフォード大学がインターネットから収集した画像群。

    * 1400万枚を超える画像を収録したデータベース。

    * 物体名は２万種以上。

## ✅ AlexNet

* 2012年、ILSVRCで優勝したSuperVisionでのモデル。

* 特徴は、ReLU、SRN、データ拡張、２枚のGPU利用。

* パラメータ数は６千万個にものぼった。（ディープラーニングのパラメータは多い）

## ✅ R-CNN（Regional CNN）

* 関心領域の切り出し（一課題）は従来の手法を用いて行う。

    ※ **バンディングボックス**（物体検出。関心領域を表す矩形領域のこと）を求める回帰問題となる。

* 検出課題についてはCNNを用いる。

* 上記組合せは、時間のかかる手法である。

## ✅ 高速RCNN（fast RCNN）

* 関心領域の切り出しと物体認識を高速に行う手法。

* 最初から最後まで深層学習でできるようになった。

## ✅ faster RCNN

* 高速RCNNが改良され、ほぼ実時間で処理できるようになったモデル。

* １６フレーム／秒程度で処理可能。

## ✅ [YOLO（You Only Look at Once）](https://qiita.com/cv_carnavi/items/68dcda71e90321574a2b)

* 検出と識別を同時に行うことで、遅延時間の短縮を実現したモデル。

## ✅ SSD（Single Shot Detector）

* YOLOより高速である。

* Faster RCNNと同等の精度を実現。

## ✅ セマンティックセグメンテーション

* R-CNNのような矩形切り出しではなく、より詳細（画素単位）な領域分割を得るモデル。

* 完全畳み込みネットワーク（FCN）のモデルがあり、すべての層が畳み込み層で構成される。（単体では画像認識を行えない）

* 同じカテゴリに属する物体はすべて同一ラベルになる。

## ✅ インスタンスセグメンテーション

* 同じカテゴリに属する物体でもすべて別ラベルにできる。

## ✅ 完全畳み込みネットワーク（FCN）

* 全ての層が畳み込み層。

## ✅ 画像データの前処理

* リサイズ、トリミング

* グレースケール化：

    カラー画像を白黒画像に変換して計算量を削減する。

* 平滑化：
    
    細かいノイズの影響を除去する。

* ヒストグラム平均：
    
    画素ごとの明るさをスケーリングする。

# 7-2.📘自然言語処理

## ✅ 関連ワード

### ✓ 言語モデル

* 「単語の意味は、その周辺の単語によって決まる」という分布仮説がある。

* [Word2Vecを理解する:Qiita](https://qiita.com/g-k/items/69afa87c73654af49d36)

### ✓ [分散表現](https://sites.google.com/site/iwanamidatascience/vol2/word-embedding)

* 記号を計算機上で扱うための方法論。

* 単語を高次元の実数ベクトルで表現する技術。

    * 単語分布図

        ![](https://sites.google.com/site/iwanamidatascience/_/rsrc/1468857206744/vol2/word-embedding/words.5k.thumbnail.png?height=600&width=600)

* 単語を固定長のベクトルで表現する。


# 参考

以下のサイトからの情報をもとに個人でまとめました。

* [【資格試験対策】ディープラーニングG検定【キーワード・ポイントまとめ】](https://sik-bug.hatenablog.com/entry/2020/05/30/103038)
