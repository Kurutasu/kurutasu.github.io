<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Trevor Hastie，統計的学習の基礎：第2章 « Memotaro</title>
	<meta name="description" content="network engineering，algorithmic trading，machine learningに関する技術メモを書きます．
">

  <link rel="icon" href="/images/h_icon.png">
  <link rel="apple-touch-icon" href="/images/h_icon.png">
  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="http://localhost:4000/old/2017-08-26-section2">
  <link rel="alternate" type="application/rss+xml" title="Memotaro" href="http://localhost:4000/feed.xml" />

  <!-- MathJax -->
  <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  <!-- BotUI -->
  <link rel="stylesheet" href="https://unpkg.com/botui/build/botui.min.css" />
  <link rel="stylesheet" href="https://unpkg.com/botui/build/botui-theme-default.css" />
</head>

  <!-- Begin Jekyll SEO tag v2.3.0 -->
<title>Trevor Hastie，統計的学習の基礎：第2章 | Memotaro</title>
<meta property="og:title" content="Trevor Hastie，統計的学習の基礎：第2章" />
<meta name="author" content="haltaro" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="network engineering，algorithmic trading，machine learningに関する技術メモを書きます．" />
<meta property="og:description" content="network engineering，algorithmic trading，machine learningに関する技術メモを書きます．" />
<link rel="canonical" href="http://localhost:4000/old/2017-08-26-section2" />
<meta property="og:url" content="http://localhost:4000/old/2017-08-26-section2" />
<meta property="og:site_name" content="Memotaro" />
<script type="application/ld+json">
{"name":null,"description":"network engineering，algorithmic trading，machine learningに関する技術メモを書きます．","author":{"@type":"Person","name":"haltaro"},"@type":"WebPage","url":"http://localhost:4000/old/2017-08-26-section2","publisher":null,"image":null,"headline":"Trevor Hastie，統計的学習の基礎：第2章","dateModified":null,"datePublished":null,"sameAs":null,"mainEntityOfPage":null,"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


  <body>

    <header class="header">
  <div class="wrapper">
    <a class="site-title" href="/">Memotaro</a>
    <nav class="site-nav">
      
        
      
        
      
        
      
        
        <a class="page-link" href="/about/">About</a>
        
      
        
        <a class="page-link" href="/category/">Category</a>
        
      
        
      
        
      
        
      
        
        <a class="page-link" href="/projects/">Projects</a>
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    </nav>
  </div>
</header>

    <div class="page-content">
      <div class="wrapper">
        <div class="col-main">
          <div class="post">

  <header class="post-header">
    <h1 class="post-title">Trevor Hastie，統計的学習の基礎：第2章</h1>
    <p class="post-meta"></p>
  </header>

  <article class="post-content">
    <p>（作成中）<a href="http://www.kyoritsu-pub.co.jp/bookdetail/9784320123625">Trevor Hastie，統計的学習の基礎 ―データマイニング・推論・予測―</a>の第2章についてメモする．本書のメモ一覧は<a href="https://haltaro.github.io/2017/08/26/the-elements-of-statistical-learning">こちら</a>．</p>

<h1 id="2-教師あり学習の概要">2. 教師あり学習の概要</h1>

<h2 id="21-導入">2.1 導入</h2>

<ul>
  <li>入力は，統計学では<strong>予測変数</strong>（predictor）や<strong>独立変数</strong>（independent variable）と，パターン認識では<strong>特徴</strong>（feature）と呼ばれる．</li>
  <li>出力は，<strong>応答変数</strong>（response variable）や<strong>従属変数</strong>（dependent variable）と呼ばれる．</li>
</ul>

<h2 id="22-変数の種類と用語">2.2 変数の種類と用語</h2>

<ul>
  <li><strong>量的変数</strong>（quantitative variable）と<strong>質的変数</strong>（qualitative variable）がある．質的変数は，<strong>カテゴリ型変数</strong>（categorical variable），<strong>離散変数</strong>（discrete variable），あるいは<strong>因子</strong>（factor）と呼ばれることがある．</li>
  <li>３つめの変数として，<strong>順序付きカテゴリ型変数</strong>（ordered categorical variable）がある．これは，値の順序にのみ意味があり，計量としての意味はない．</li>
  <li>本書では，入力変数を<script type="math/tex">X</script>，量的な出力変数を<script type="math/tex">Y</script>，質的な出力変数を<script type="math/tex">G</script>と表す．<script type="math/tex">X</script>の<script type="math/tex">i</script>番目の観測値は，ベクトルでもスカラーでも，イタリック体で<script type="math/tex">x_i</script>と表す．一方で，全てのベクトル<script type="math/tex">x_i</script>の<script type="math/tex">j</script>番目の要素をまとめたベクトルは，立体で<script type="math/tex">\mathrm{x}_j</script>と表す．</li>
  <li>学習とは，入力ベクトル<script type="math/tex">X</script>が与えられたとき，出力<script type="math/tex">Y</script>の良い予測値<script type="math/tex">\hat{Y}</script>を求めること．</li>
</ul>

<h2 id="23-予測のための二つの簡単なアプローチ最小二乗法と最近傍法">2.3 予測のための二つの簡単なアプローチ：最小二乗法と最近傍法</h2>

<ul>
  <li><strong>線形モデル</strong>は，入力ベクトル<script type="math/tex">X^{T}=\left(X_1, X_2, ..., X_p\right)</script>に対し，出力<script type="math/tex">Y</script>を<script type="math/tex">\hat{Y}=X^{T}\hat{\beta}</script>で予測するモデルである．</li>
  <li>線形モデルにおける訓練データの当てはめには，<strong>最小二乗法</strong>が用いられる．最小二乗法では，残差二乗和<script type="math/tex">\mathrm{RSS}(\beta)=\left(\mathbf{y} - \mathbf{X} \beta \right)^{T}\left(\mathbf{y} - \mathbf{X} \beta \right)</script>を最小化する<script type="math/tex">\beta</script>を求める．ここで，<script type="math/tex">\mathbf{X}</script>は各行が入力ベクトル<script type="math/tex">X</script>である<script type="math/tex">N \times p</script>行列，<script type="math/tex">\mathbf{y}</script>は各行が出力<script type="math/tex">y</script>である<script type="math/tex">N</script>次元ベクトルである．</li>
  <li>上式を<script type="math/tex">\beta</script>について偏微分して導出される<script type="math/tex">\mathbf{X}^{T}\left(\mathbf{y}-\mathbf{X}\beta\right)=0</script>を<strong>正規方程式</strong>と呼ぶ．これを解くと，<script type="math/tex">\hat{\beta} = \left( \mathbf{X}^{T} \mathbf{X} \right)^{-1} \mathbf{X}^{T} \mathbf{y}</script>を得る．</li>
  <li><strong>最近傍法</strong>は，入力<script type="math/tex">x</script>に対し，出力<script type="math/tex">Y</script>を<script type="math/tex">\hat{Y}=\frac{1}{k}\sum_{x_{i} \in N_{k}(x)}y_{i}</script>で予測するモデルである．ここで，<script type="math/tex">N_{k}(x)</script>は訓練データのうち<script type="math/tex">x</script>に近い<script type="math/tex">k</script>個の点<script type="math/tex">x_{i}</script>によって定義される近傍集合．分類問題に<script type="math/tex">k=1</script>の最近傍法を用いると，訓練データの<a href="https://ja.wikipedia.org/wiki/%E3%83%9C%E3%83%AD%E3%83%8E%E3%82%A4%E5%9B%B3"><strong>ボロノイ分割</strong></a>が得られる．</li>
  <li>最近傍法のパラメータ<script type="math/tex">k</script>を最小二乗法で求めることはできない．</li>
  <li>線形モデルは分散が小さく，バイアスが大きい．一方で最近傍法は，分散が大きく，バイアスが小さい．</li>
  <li>線形モデルおよび最近傍法の拡張：
    <ul>
      <li><strong>カーネル平滑化</strong>：最近傍法では重みが0か1かの離散値だったが，カーネル平滑化では連続値に拡張する．</li>
      <li><strong>局所線形モデル</strong>：最近傍法では局所領域<script type="math/tex">N</script>が定数モデルだったが，局所線形モデルでは線形モデルに拡張する．</li>
      <li><strong>基底展開</strong>：入力モデルを基底展開することで，線形モデルを拡張する．</li>
      <li><strong>射影追跡法</strong>や<strong>ニューラルネットワーク</strong>は，非線形返還と線形モデルを結合したものと考えられる．</li>
    </ul>
  </li>
</ul>

<h2 id="24-統計的決定理論">2.4 統計的決定理論</h2>

<ul>
  <li>確率入力ベクトル<script type="math/tex">X \in \mathbb{R}^{p}</script>と確率出力変数<script type="math/tex">Y \in \mathbb{R}</script>が同時確率分布<script type="math/tex">\mathrm{Pr}(X,Y)</script>に従うと仮定する．目的は，入力<script type="math/tex">X</script>に対して出力<script type="math/tex">Y</script>を予測する関数<script type="math/tex">f(X)</script>を見つけること．</li>
  <li><script type="math/tex">L\left(Y,f(X)\right)</script>を<strong>損失関数</strong>（Loss function）とする．最も頻繁に利用されるのは，<strong>二乗誤差損失</strong>（Least square loss）<script type="math/tex">L\left(Y,f(X)\right)=\left(Y-f(X)\right)^2</script>．</li>
  <li>期待（二乗）予測誤差（expected prediction error）<script type="math/tex">\mathrm{EPE}(f)=\mathrm{E}\left(Y-f(X)\right)</script>を最小化する<script type="math/tex">f</script>は条件付き期待値<script type="math/tex">\mathrm{E}(Y\mid X=x)</script>であり，これは<strong>回帰関数</strong>とも呼ばれる．</li>
  <li>最近傍法は，訓練データを用いて条件付き期待値を直接推定する方法と解釈できる．具体的には，<script type="math/tex">\hat{f}(x)=\mathrm{Ave}\left(y_i \mid x_i \in N_k(x) \right)</script>．ここでは，以下の近似が行われていることに注意すべき：
    <ul>
      <li>期待値を標本データの平均で近似している．</li>
      <li>点<script type="math/tex">x</script>に対する条件付けではなく，それを緩和した近傍<script type="math/tex">N_k(x)</script>で条件付けしている．</li>
    </ul>
  </li>
  <li>訓練データ数<script type="math/tex">N</script>および近傍数<script type="math/tex">k</script>を十分大きくすれば，<script type="math/tex">\hat{f}(x)</script>は条件付き期待値に収束する．しかし，次元数<script type="math/tex">p</script>が大きいとき，十分な数のデータを得ることが困難になるし，収束も遅くなる．</li>
  <li>一方，線形モデル<script type="math/tex">f(x)=x^{T}\beta</script>のとき，解<script type="math/tex">\beta</script>は解析的に求められる：<script type="math/tex">\beta=[\mathrm{E}(XX^{T})]^{-1}\mathrm{E}(XY)</script>．</li>
  <li>最近傍法も最小二乗法も，条件付き期待値の近似である．最近合法では<script type="math/tex">f(x)</script>を局所的な定数関数で近似し，最小二乗法では<script type="math/tex">f(x)</script>を大域的な線形関数で近似している．</li>
  <li>ちなみに，<script type="math/tex">L_2</script>損失関数を<script type="math/tex">L_1</script>損失関数に変更すると，解は条件付き中央値<script type="math/tex">\hat{f}(x)=\mathrm{median}(Y \mid X=x)</script>となる．</li>
  <li>出力がカテゴリ型変数<script type="math/tex">G</script>の場合，損失関数は<script type="math/tex">K \times K</script>行列<script type="math/tex">\mathbf{L}</script>となる．損失行列<script type="math/tex">\mathbf{L}</script>の要素<script type="math/tex">(k,l)</script>は，クラス<script type="math/tex">\mathcal{G}_k</script>を<script type="math/tex">\mathcal{G}_l</script>に分類した際のペナルティを表す．損失行列<script type="math/tex">\mathbf{L}</script>が<script type="math/tex">\{0, 1\}</script>のとき，<script type="math/tex">0/1</script>損失関数と呼ばれる．</li>
  <li>期待予測誤差は<script type="math/tex">\mathrm{EPE}</script><script type="math/tex">=\mathrm{E}\left[L\left(G, \hat{G}\right)\right]</script><script type="math/tex">=\mathrm{E}_{X}\sum_{k=1}^{K}L\left(\mathcal{G}_k, \hat{G}(X)\right)\mathrm{Pr}(\mathcal{G}_k\mid X)</script>となるので，</li>
</ul>

<h2 id="25-高次元での局所的手法">2.5 高次元での局所的手法</h2>

<h2 id="26-統計モデル教師あり学習関数近似">2.6 統計モデル，教師あり学習，関数近似</h2>

<h2 id="27-構造化回帰モデル">2.7 構造化回帰モデル</h2>

<h2 id="28-制限付き推定法">2.8 制限付き推定法</h2>

<h2 id="29-モデルの選択とバイアスと分散のトレードオフ">2.9 モデルの選択と，バイアスと分散のトレードオフ</h2>

  </article>
  
  




</div>

        </div>
        <div class="col-second">
          <div class="col-box col-box-author">
  <img class="avatar" src="//www.gravatar.com/avatar/546804f938d574fc5141fc1431066d29" alt="haltaro">
  <div class="col-box-title name">haltaro</div>
  <p>柴犬を愛でる研究員</p>
  <p class="contact">
    
    <a href="https://github.com/haltaro">GitHub</a>
    
    
    
		<a href="https://qiita.com/haltaro">Qiita</a>
    
    
		<a href="https://kaggle.com/haltaro">Kaggle</a>
    

    
    <a href="mailto:mail.to.haltaro@gmail.com">Email</a>
    
  </p>
</div>

<div class="col-box">
  <div class="col-box-title">Newest Posts</div>
  <ul class="post-list">
    
      <li><a class="post-link" href="/2018/01/07/digital-marketing-retire">CourceraのDigital marketing講座を退会した</a></li>
    
      <li><a class="post-link" href="/2018/01/07/digital-marketing-4">Courcera，Marketing in a digital world：4. Price</a></li>
    
      <li><a class="post-link" href="/2018/01/02/dmm-english">DMM英会話受講メモ：2018年1月</a></li>
    
      <li><a class="post-link" href="/2017/12/30/chatbot">チャットボットを作ろう：2．JekyllサイトでBotUI</a></li>
    
      <li><a class="post-link" href="/2017/12/25/chatbot">チャットボットを作ろう：1．調査</a></li>
    
  </ul>
</div>

<div class="col-box post-toc hide">
  <div class="col-box-title">TOC</div>
</div>

        </div>
      </div>
    </div>

    <footer class="footer">
<div class="wrapper">
&copy; 2017 haltaro
</div>
</footer>

<script src="/js/easybook.js"></script>

    


  </body>

</html>
